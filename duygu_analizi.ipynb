{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbc712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Veri setinin dosya yolu. Eğer dosya aynı klasördeyse sadece adı yeterli.\n",
    "# Eğer farklı bir yerde ise, tam dosya yolunu belirtmelisin.\n",
    "file_path = 'IMDB Dataset.csv'\n",
    "\n",
    "try:\n",
    "    # CSV dosyasını oku\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(\"Veri Seti Başarıyla Yüklendi!\")\n",
    "    print(\"\\nİlk 5 satır:\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\nVeri Seti Bilgileri:\")\n",
    "    df.info()\n",
    "\n",
    "    print(\"\\nDuygu Dağılımı:\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "\n",
    "    print(\"\\nMetin Sütunu İçin Bazı İstatistikler (Karakter Sayısı):\")\n",
    "    df['text_length'] = df['review'].apply(len)\n",
    "    print(df['text_length'].describe())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Hata: '{file_path}' dosyası bulunamadı. Lütfen dosya yolunu kontrol edin veya dosyanın aynı klasörde olduğundan emin olun.\")\n",
    "except Exception as e:\n",
    "    print(f\"Bir hata oluştu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Metindeki HTML etiketlerini kaldırır.\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# 'review' sütununa HTML etiketlerini temizleme fonksiyonunu uygula\n",
    "df['review_cleaned'] = df['review'].apply(remove_html_tags)\n",
    "\n",
    "# Temizlenmiş ilk 5 yorumu ve orijinal yorumları karşılaştır\n",
    "print(\"Orijinal ve Temizlenmiş Yorumların İlk 5 Satırı:\")\n",
    "print(df[['review', 'review_cleaned']].head())\n",
    "\n",
    "# Değişikliğin olup olmadığını görmek için rastgele bir yorumu kontrol edebiliriz\n",
    "# Örneğin, 3. yorumu alalım (indeksi 2)\n",
    "print(\"\\n-----------------------------------------------------\")\n",
    "print(\"Rastgele Bir Yorumun Orijinal ve Temizlenmiş Hali (Örnek Indeks 2):\")\n",
    "print(f\"Orijinal: {df['review'].iloc[2]}\")\n",
    "print(f\"Temizlenmiş: {df['review_cleaned'].iloc[2]}\")\n",
    "print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re # Eğer regex kullanacaksan\n",
    "\n",
    "# Küçük harfe çevirme\n",
    "df['review_cleaned'] = df['review_cleaned'].str.lower()\n",
    "\n",
    "# Noktalama işaretlerini kaldırma (bir yöntem)\n",
    "# df['review_cleaned'] = df['review_cleaned'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Noktalama işaretlerini kaldırma (başka bir yöntem, regex ile)\n",
    "df['review_cleaned'] = df['review_cleaned'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "\n",
    "# Temizlenmiş (küçük harfe çevrilmiş ve noktalama işaretleri kaldırılmış) ilk 5 yorumu kontrol et\n",
    "print(\"Küçük Harf ve Noktalama Temizliği Sonrası İlk 5 Yorum:\")\n",
    "print(df[['review_cleaned']].head())\n",
    "\n",
    "# Rastgele bir örnek yorumu tekrar kontrol edebilirsin\n",
    "print(\"\\n-----------------------------------------------------\")\n",
    "print(\"Rastgele Bir Yorumun Noktalama Temizliği Sonrası Hali (Örnek Indeks 2):\")\n",
    "print(f\"Temizlenmiş (Yeni Hali): {df['review_cleaned'].iloc[2]}\")\n",
    "print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19981e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# İngilizce durak kelimeleri yükle\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Metindeki durak kelimeleri kaldırır.\"\"\"\n",
    "    words = text.split() # Metni kelimelere ayır (tokenization)\n",
    "    # Her kelimeyi kontrol et ve eğer durak kelime listesinde değilse, o kelimeyi koru\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words) # Kalan kelimeleri tekrar birleştir\n",
    "\n",
    "# 'review_cleaned' sütununa durak kelime temizleme fonksiyonunu uygula\n",
    "df['review_cleaned'] = df['review_cleaned'].apply(remove_stopwords)\n",
    "\n",
    "# Temizlenmiş ilk 5 yorumu kontrol et\n",
    "print(\"Durak Kelimeler Temizliği Sonrası İlk 5 Yorum:\")\n",
    "print(df[['review_cleaned']].head())\n",
    "\n",
    "# Rastgele bir örnek yorumu tekrar kontrol et\n",
    "print(\"\\n-----------------------------------------------------\")\n",
    "print(\"Rastgele Bir Yorumun Durak Kelimeler Temizliği Sonrası Hali (Örnek Indeks 2):\")\n",
    "print(f\"Temizlenmiş (Yeni Hali): {df['review_cleaned'].iloc[2]}\")\n",
    "print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaa884f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Adım 4: Metin Temsili (TF-IDF Vectorization) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# TF-IDF Vectorizer nesnesini oluştur\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# max_features: En sık geçen 10.000 kelimeyi al (önemli kelimeleri sınırlamak için)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# min_df: Bir kelime en az 5 belgede geçmiyorsa göz ardı et (çok nadir kelimeleri elemek için)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# max_df: Bir kelime belgelerin %70'inden fazlasında geçiyorsa göz ardı et (çok yaygın kelimeleri elemek için)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"\\n--- Adım 4: Metin Temsili (TF-IDF Vectorization) ---\")\n",
    "\n",
    "# TF-IDF Vectorizer nesnesini oluştur\n",
    "# max_features: En sık geçen 10.000 kelimeyi al (önemli kelimeleri sınırlamak için)\n",
    "# min_df: Bir kelime en az 5 belgede geçmiyorsa göz ardı et (çok nadir kelimeleri elemek için)\n",
    "# max_df: Bir kelime belgelerin %70'inden fazlasında geçiyorsa göz ardı et (çok yaygın kelimeleri elemek için)\n",
    "vectorizer = TfidfVectorizer(max_features=10000, min_df=5, max_df=0.7)\n",
    "\n",
    "# Temizlenmiş yorumları (df['review_cleaned']) TF-IDF vektörlerine dönüştür\n",
    "# X artık modelin kullanacağı sayısal özellik matrisi olacak\n",
    "X = vectorizer.fit_transform(df['review_cleaned'])\n",
    "\n",
    "# Oluşan TF-IDF matrisinin boyutlarını kontrol et\n",
    "# Çıktı (yorum sayısı, benzersiz ve filtrelenmiş kelime sayısı) şeklinde olacaktır.\n",
    "print(\"TF-IDF Matrisinin Boyutları (Yorum Sayısı, Benzersiz Kelime Sayısı):\", X.shape)\n",
    "\n",
    "# İsteğe bağlı: Oluşturulan kelime dağarcığının (vocabulary) ilk birkaç elemanını görmek\n",
    "# Eğer istersen bu satırı yorumdan çıkarıp kelimeleri görebilirsin.\n",
    "# print(\"\\nVectorizer Kelime Dağarcığından İlk 10 Kelime:\")\n",
    "# print(vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "print(\"\\nMetin temsil adımı tamamlandı.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
